{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rajat95gupta/smartphone-image-denoising-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbxR90ayJS4f",
        "outputId": "f47706de-ff26-4a7f-de12-51a5c14d43c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rajat95gupta/smartphone-image-denoising-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.16G/6.16G [00:55<00:00, 120MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rajat95gupta/smartphone-image-denoising-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/rajat95gupta/smartphone-image-denoising-dataset/versions/1/SIDD_Small_sRGB_Only/Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r47iOlYRKNqy",
        "outputId": "047f694d-80cb-4f90-e210-01b523e58e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0001_001_S6_00100_00060_3200_L\t0104_005_S6_03200_01600_4400_L\n",
            "0002_001_S6_00100_00020_3200_N\t0105_005_GP_00100_00100_4400_N\n",
            "0003_001_S6_00100_00060_3200_H\t0106_005_GP_00400_00400_4400_N\n",
            "0004_001_S6_00100_00060_4400_L\t0107_005_GP_01600_01600_4400_N\n",
            "0005_001_S6_00100_00060_4400_N\t0108_005_GP_06400_06400_4400_N\n",
            "0006_001_S6_00100_00060_4400_H\t0110_005_IP_00100_00100_5500_L\n",
            "0007_001_S6_00100_00100_5500_L\t0111_005_IP_00400_00400_5500_L\n",
            "0008_001_S6_00100_00100_5500_N\t0113_005_IP_01600_01520_5500_L\n",
            "0010_001_S6_00800_00350_3200_N\t0114_005_IP_00100_00200_5500_N\n",
            "0011_001_S6_00800_00500_5500_L\t0115_005_IP_00400_00750_5500_N\n",
            "0012_001_S6_00800_00500_5500_N\t0116_005_IP_00800_01520_5500_N\n",
            "0013_001_S6_03200_01250_3200_L\t0117_005_IP_01600_04160_5500_N\n",
            "0014_001_S6_03200_01250_3200_N\t0118_006_N6_00100_00025_3200_L\n",
            "0015_001_S6_03200_01600_5500_L\t0120_006_N6_01600_00400_3200_L\n",
            "0016_001_S6_03200_01600_5500_N\t0121_006_N6_03200_01000_3200_L\n",
            "0017_001_GP_00100_00060_5500_N\t0122_006_G4_00100_00050_3200_N\n",
            "0018_001_GP_00100_00160_5500_L\t0123_006_G4_00400_00160_3200_N\n",
            "0019_001_GP_00800_00640_5500_L\t0125_006_S6_00100_00050_4400_L\n",
            "0020_001_GP_00800_00350_5500_N\t0126_006_S6_00400_00200_4400_L\n",
            "0022_001_N6_00100_00060_5500_N\t0127_006_S6_01600_00800_4400_L\n",
            "0023_001_N6_00800_00350_5500_N\t0129_006_GP_00100_00100_4400_N\n",
            "0025_001_G4_00100_00060_5500_L\t0130_006_GP_00400_00400_4400_N\n",
            "0027_001_G4_00800_00350_5500_L\t0132_006_GP_00100_00200_5500_L\n",
            "0028_001_IP_00100_00160_5500_N\t0133_006_GP_00800_01600_5500_L\n",
            "0029_001_IP_00800_01000_5500_N\t0134_006_IP_00100_00100_5500_N\n",
            "0030_001_IP_01600_02000_5500_N\t0135_006_IP_00400_00400_5500_N\n",
            "0032_001_IP_00800_01000_3200_N\t0136_006_IP_00800_00800_5500_N\n",
            "0033_001_IP_00100_00160_3200_N\t0137_006_IP_01600_01600_5500_N\n",
            "0034_002_GP_00100_00160_3200_N\t0138_006_IP_00100_00100_3200_L\n",
            "0035_002_GP_00800_00350_3200_N\t0139_006_IP_00200_00200_3200_L\n",
            "0036_002_GP_06400_03200_3200_N\t0140_006_IP_00800_00800_3200_L\n",
            "0038_002_GP_00800_00640_3200_L\t0142_007_N6_00100_00100_4400_N\n",
            "0039_002_IP_00100_00180_5500_L\t0144_007_N6_01600_01600_4400_N\n",
            "0040_002_IP_00800_02000_5500_L\t0145_007_N6_03200_03200_4400_N\n",
            "0042_002_IP_01600_03100_5500_N\t0146_007_N6_00400_00400_4400_N\n",
            "0043_002_IP_00800_01520_5500_N\t0147_007_G4_00100_00100_4400_L\n",
            "0044_002_IP_00100_00180_5500_N\t0149_007_G4_00800_00800_4400_L\n",
            "0045_002_G4_00100_00060_3200_L\t0150_007_S6_00100_00100_5500_L\n",
            "0047_002_G4_00800_00640_3200_L\t0151_007_S6_00800_00800_5500_L\n",
            "0048_002_N6_00100_00100_5500_L\t0152_007_S6_01600_01600_5500_L\n",
            "0050_002_N6_03200_03200_5500_L\t0154_007_S6_00400_00400_5500_L\n",
            "0051_002_S6_00100_00060_5500_N\t0155_007_GP_00100_00100_5500_N\n",
            "0052_002_S6_01600_01000_5500_N\t0156_007_GP_00800_00800_5500_N\n",
            "0054_003_N6_00100_00160_5500_N\t0157_007_GP_01600_01600_5500_N\n",
            "0055_003_N6_00800_01000_5500_N\t0159_007_IP_00100_00100_3200_L\n",
            "0057_003_G4_00100_00125_5500_L\t0160_007_IP_00400_00400_3200_L\n",
            "0059_003_G4_00800_01000_5500_L\t0161_007_IP_00800_00800_3200_L\n",
            "0060_003_S6_00100_00100_4400_L\t0163_007_IP_00100_00100_3200_N\n",
            "0062_003_S6_03200_02500_4400_L\t0164_007_IP_00400_00400_3200_N\n",
            "0063_003_GP_00100_00100_4400_N\t0165_007_IP_00800_00800_3200_N\n",
            "0064_003_GP_01600_01600_4400_N\t0166_007_IP_01600_01600_3200_N\n",
            "0065_003_GP_10000_08460_4400_N\t0167_008_N6_00100_00050_4400_L\n",
            "0066_003_GP_00100_00200_3200_L\t0168_008_N6_00400_00200_4400_L\n",
            "0068_003_IP_00200_00400_3200_N\t0169_008_N6_00800_00400_4400_L\n",
            "0069_003_IP_01000_02000_3200_N\t0170_008_N6_01600_00800_4400_L\n",
            "0070_003_IP_02000_04000_3200_N\t0172_008_G4_00100_00100_4400_N\n",
            "0072_003_IP_01000_02000_5500_L\t0173_008_G4_00400_00400_4400_N\n",
            "0073_003_IP_00200_01000_5500_L\t0175_008_S6_00100_00025_5500_L\n",
            "0075_004_N6_00800_00080_3200_L\t0177_008_S6_00800_00200_5500_L\n",
            "0076_004_N6_03200_00320_3200_L\t0178_008_S6_01600_00400_5500_L\n",
            "0077_004_G4_00100_00025_3200_N\t0179_008_S6_03200_00800_5500_L\n",
            "0078_004_G4_00200_00050_3200_N\t0180_008_GP_00100_00100_5500_N\n",
            "0080_004_S6_00200_00050_3200_N\t0181_008_GP_00800_00800_5500_N\n",
            "0081_004_S6_00800_00160_4400_L\t0182_008_GP_03200_03200_5500_N\n",
            "0083_004_GP_00050_00020_4400_N\t0184_008_IP_00100_00100_3200_L\n",
            "0084_004_GP_00200_00100_4400_N\t0185_008_IP_00400_00400_3200_L\n",
            "0086_004_GP_00100_00100_5500_L\t0186_008_IP_00800_00800_3200_L\n",
            "0087_004_GP_00800_00640_5500_L\t0188_008_IP_00100_00100_3200_N\n",
            "0088_004_IP_00100_00050_5500_N\t0189_008_IP_00400_00400_3200_N\n",
            "0089_004_IP_00500_00250_5500_N\t0190_008_IP_00800_00800_3200_N\n",
            "0090_004_IP_01600_00750_5500_N\t0191_008_IP_01600_01600_3200_N\n",
            "0091_004_IP_00320_00080_3200_L\t0192_009_IP_00100_00200_3200_N\n",
            "0092_004_IP_00640_00125_3200_L\t0193_009_IP_00800_02000_3200_N\n",
            "0094_005_N6_00100_00050_3200_L\t0194_009_IP_01600_04000_3200_N\n",
            "0096_005_N6_01600_01000_3200_L\t0195_009_IP_01600_04000_5500_L\n",
            "0097_005_N6_03200_02000_3200_L\t0196_009_IP_00800_02000_5500_L\n",
            "0098_005_G4_00100_00050_3200_N\t0197_009_IP_00100_00200_5500_L\n",
            "0099_005_G4_00400_00200_3200_N\t0198_010_GP_00100_00200_5500_N\n",
            "0101_005_S6_00100_00050_4400_L\t0199_010_GP_00800_01600_5500_N\n",
            "0102_005_S6_00400_00200_4400_L\t0200_010_GP_01600_03200_5500_N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pillow tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGhpedxiIwYD",
        "outputId": "ef99cf95-c01e-4c8a-f775-9b9cdb418385"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================================================\n",
        "# 1️⃣ RIDNet Model Definition\n",
        "# ===============================================================\n",
        "class FA_Block(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.relu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        attention = self.sigmoid(residual)\n",
        "        return x * attention\n",
        "\n",
        "class RIDNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, channels=64, num_fa_blocks=4, residual_scale=0.1):\n",
        "        super().__init__()\n",
        "        self.residual_scale = residual_scale\n",
        "        self.entry = nn.Conv2d(in_channels, channels, kernel_size=3, padding=1)\n",
        "        self.blocks = nn.Sequential(*[FA_Block(channels) for _ in range(num_fa_blocks)])\n",
        "        self.exit = nn.Conv2d(channels, in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.entry(x)\n",
        "        features = self.blocks(features)\n",
        "        residual = self.exit(features) * self.residual_scale\n",
        "        return x - residual\n",
        "\n",
        "# ===============================================================\n",
        "# 2️⃣ SIDD Dataset Definition\n",
        "# ===============================================================\n",
        "class SIDDataset(Dataset):\n",
        "    def __init__(self, file_pairs, patch_size=128, augment=True):\n",
        "        self.files = file_pairs\n",
        "        self.patch_size = patch_size\n",
        "        self.augment = augment\n",
        "        self.to_tensor = T.ToTensor()\n",
        "\n",
        "        assert len(self.files) > 0, \"No file pairs provided!\"\n",
        "\n",
        "    def random_crop(self, img1, img2):\n",
        "        i, j, h, w = T.RandomCrop.get_params(img1, output_size=(self.patch_size, self.patch_size))\n",
        "        return T.functional.crop(img1, i, j, h, w), T.functional.crop(img2, i, j, h, w)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        noisy_path, gt_path = self.files[idx]\n",
        "        noisy = Image.open(noisy_path).convert('RGB')\n",
        "        gt = Image.open(gt_path).convert('RGB')\n",
        "\n",
        "        noisy, gt = self.random_crop(noisy, gt)\n",
        "\n",
        "        if self.augment and torch.rand(1) > 0.5:\n",
        "            noisy = T.functional.hflip(noisy)\n",
        "            gt = T.functional.hflip(gt)\n",
        "\n",
        "        return self.to_tensor(noisy), self.to_tensor(gt)\n",
        "\n",
        "# ===============================================================\n",
        "# 3️⃣ Prepare file pairs & train/val split\n",
        "# ===============================================================\n",
        "DATA_DIR = \"/root/.cache/kagglehub/datasets/rajat95gupta/smartphone-image-denoising-dataset/versions/1/SIDD_Small_sRGB_Only/Data\"\n",
        "\n",
        "# ── 3.1 Collect all subdirectories (scenes)\n",
        "subdirs = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "print(f\"Found {len(subdirs)} subdirectories (scenes)\")\n",
        "print(\"Examples:\", subdirs[:10])\n",
        "\n",
        "# ── 3.2 Build file pairs from each subdirectory\n",
        "pairs = []\n",
        "for subdir in subdirs:\n",
        "    subdir_path = os.path.join(DATA_DIR, subdir)\n",
        "    files = os.listdir(subdir_path)\n",
        "\n",
        "    # Find noisy and ground truth files in this subdirectory\n",
        "    noisy_files = [f for f in files if \"NOISY\" in f and f.endswith(\".PNG\")]\n",
        "    gt_files = [f for f in files if \"GT\" in f and f.endswith(\".PNG\")]\n",
        "\n",
        "    # Create pairs for this scene\n",
        "    for noisy_file in noisy_files:\n",
        "        for gt_file in gt_files:\n",
        "            # Make sure they belong to the same image\n",
        "            # In SIDD, each scene has one GT and one noisy image\n",
        "            noisy_path = os.path.join(subdir_path, noisy_file)\n",
        "            gt_path = os.path.join(subdir_path, gt_file)\n",
        "            pairs.append((noisy_path, gt_path))\n",
        "\n",
        "print(f\"Total pairs found: {len(pairs)}\")\n",
        "\n",
        "if len(pairs) == 0:\n",
        "    # Alternative approach: look for specific file patterns\n",
        "    print(\"Trying alternative file discovery...\")\n",
        "    for subdir in subdirs:\n",
        "        subdir_path = os.path.join(DATA_DIR, subdir)\n",
        "        files = os.listdir(subdir_path)\n",
        "\n",
        "        # Look for any PNG files and pair them logically\n",
        "        png_files = [f for f in files if f.endswith(\".PNG\")]\n",
        "        if len(png_files) == 2:  # Should have exactly 2 files per directory\n",
        "            # Determine which is noisy and which is GT\n",
        "            noisy_file = next((f for f in png_files if \"NOISY\" in f), None)\n",
        "            gt_file = next((f for f in png_files if \"GT\" in f), None)\n",
        "\n",
        "            if noisy_file and gt_file:\n",
        "                pairs.append((os.path.join(subdir_path, noisy_file),\n",
        "                             os.path.join(subdir_path, gt_file)))\n",
        "\n",
        "    print(f\"Pairs found with alternative method: {len(pairs)}\")\n",
        "\n",
        "# ── 3.3 Show some examples\n",
        "if pairs:\n",
        "    print(\"Example pairs:\")\n",
        "    for i, (noisy, gt) in enumerate(pairs[:5]):\n",
        "        print(f\"  {i+1}. Noisy: {os.path.basename(noisy)}\")\n",
        "        print(f\"     GT:   {os.path.basename(gt)}\")\n",
        "\n",
        "# ── 3.4 Shuffle & split\n",
        "random.shuffle(pairs)\n",
        "split = int(0.8 * len(pairs))\n",
        "train_pairs = pairs[:split]\n",
        "val_pairs = pairs[split:]\n",
        "\n",
        "print(f\"Training pairs: {len(train_pairs)}\")\n",
        "print(f\"Validation pairs: {len(val_pairs)}\")\n",
        "\n",
        "if len(pairs) == 0:\n",
        "    raise ValueError(\"No file pairs found! Check the dataset structure.\")\n",
        "\n",
        "# ── 3.5 Datasets & loaders\n",
        "train_dataset = SIDDataset(train_pairs, patch_size=128, augment=True)\n",
        "val_dataset = SIDDataset(val_pairs, patch_size=128, augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ===============================================================\n",
        "# 4️⃣ Model, optimizer & loss\n",
        "# ===============================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = RIDNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# ===============================================================\n",
        "# 5️⃣ Training loop\n",
        "# ===============================================================\n",
        "EPOCHS = 80\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # ── 5.1 Training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for noisy, gt in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Training]\", leave=False):\n",
        "        noisy, gt = noisy.to(device), gt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(noisy)\n",
        "        loss = criterion(output, gt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * noisy.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.6f}\")\n",
        "\n",
        "    # ── 5.2 Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for noisy, gt in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "            noisy, gt = noisy.to(device), gt.to(device)\n",
        "            output = model(noisy)\n",
        "            loss = criterion(output, gt)\n",
        "            val_loss += loss.item() * noisy.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Validation Loss = {val_loss:.6f}\")\n",
        "\n",
        "    # ── 5.3 Save best model\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"/content/ridnet_sidd.pth\")\n",
        "        print(f\"✅ Model saved! Best val loss: {best_loss:.6f}\")\n",
        "\n",
        "print(\"🎉 Training complete! Model saved as /content/ridnet_sidd.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9cETHt4dvi-",
        "outputId": "537af649-ce0c-4d0d-ca75-42b9df13cb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 160 subdirectories (scenes)\n",
            "Examples: ['0068_003_IP_00200_00400_3200_N', '0010_001_S6_00800_00350_3200_N', '0025_001_G4_00100_00060_5500_L', '0139_006_IP_00200_00200_3200_L', '0169_008_N6_00800_00400_4400_L', '0063_003_GP_00100_00100_4400_N', '0184_008_IP_00100_00100_3200_L', '0113_005_IP_01600_01520_5500_L', '0200_010_GP_01600_03200_5500_N', '0104_005_S6_03200_01600_4400_L']\n",
            "Total pairs found: 160\n",
            "Example pairs:\n",
            "  1. Noisy: NOISY_SRGB_010.PNG\n",
            "     GT:   GT_SRGB_010.PNG\n",
            "  2. Noisy: NOISY_SRGB_010.PNG\n",
            "     GT:   GT_SRGB_010.PNG\n",
            "  3. Noisy: NOISY_SRGB_010.PNG\n",
            "     GT:   GT_SRGB_010.PNG\n",
            "  4. Noisy: NOISY_SRGB_010.PNG\n",
            "     GT:   GT_SRGB_010.PNG\n",
            "  5. Noisy: NOISY_SRGB_010.PNG\n",
            "     GT:   GT_SRGB_010.PNG\n",
            "Training pairs: 128\n",
            "Validation pairs: 32\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/80 [Training]:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/80 [Training]:  25%|██▌       | 2/8 [01:42<04:53, 48.87s/it]"
          ]
        }
      ]
    }
  ]
}
